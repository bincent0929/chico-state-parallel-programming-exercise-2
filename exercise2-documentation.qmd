## 1.

### a.

In running my `edited-pthread.c` program (located in `simplethread/`), the only difference between runs is which thread presents its sum first.

Run #1:
```bash
Thread idx=0, sum[0...0]=5050
Thread idx=2, sum[0...2]=45150
Thread idx=3, sum[0...3]=80200
Thread idx=4, sum[0...4]=125250
Thread idx=1, sum[0...1]=20100
Thread idx=5, sum[0...5]=180300
Thread idx=6, sum[0...6]=245350
Thread idx=7, sum[0...7]=320400
TEST COMPLETE
```

Run #2:
```bash
Thread idx=0, sum[0...0]=5050
Thread idx=3, sum[0...3]=80200
Thread idx=2, sum[0...2]=45150
Thread idx=1, sum[0...1]=20100
Thread idx=4, sum[0...4]=125250
Thread idx=7, sum[0...7]=320400
Thread idx=5, sum[0...5]=180300
Thread idx=6, sum[0...6]=245350
```

Run #3:
```bash
$ ./edited-pthread 
Thread idx=0, sum[0...0]=5050
Thread idx=2, sum[0...2]=45150
Thread idx=1, sum[0...1]=20100
Thread idx=3, sum[0...3]=80200
Thread idx=5, sum[0...5]=180300
Thread idx=6, sum[0...6]=245350
Thread idx=7, sum[0...7]=320400
Thread idx=4, sum[0...4]=125250
TEST COMPLETE
```

``` {python}
n = 100
c_calculated_sumRange = sorted([5050, 45150, 20100, 80200, 180300, 245350, 320400, 125250])

for sumOfRange in c_calculated_sumRange:
    formula_calculation = (n * (n+1)) / 2
    if formula_calculation == sumOfRange:
        n+=100
        continue
    else:
        print(f"The sums for the range 0..{n} was not correct.")
        break

if n == 900:
    print("All the values were correct!")
```

### b.

There's A LOT of output that comes from the 1...1*10^6 program. So I can't really see the order exactly, but it does appear that the thread at index 4 finished last. Which makes it clear that the threads don't operate in sequential, numbered order.

Based on the output from `edit-sumnums.c`:
```bash
TEST COMPLETE: gsum[0]=4999950000, gsum[1]=14999950000, gsum[2]=24999950000, gsum[3]=34999950000, gsum[4]=44999950000, gsum[5]=54999950000, gsum[6]=64999950000, gsum[7]=74999950000, gsum[8]=84999950000, gsum[9]=94999950000, gsumall=500000500000
TEST COMPLETE: gsumall=500000500000, [n[n+1]]/2=500000500000
```

From the output, the outcome of each thread completing their job possibly out of numerical sequence does not have an impact on the final sum.

### c.

So I went back and timed my `edited-sumnums.c` program that calculated 1...1*10^6.

The result was:
```bash
real    0m43.502s
user    0m0.474s
sys     0m1.994s
```

And my `edited-openmp-sumnums.c` implementation resulted in:
```bash
real    0m41.644s
user    0m0.199s
sys     0m0.645s
```

Which I'm surprised by. When running it, it seems much faster than the Pthread implementation.

In terms of order, the OpenMP implementation does start on the first index and go through each sequentially from `idx = 0...9`.

Yes, I could verify whether the subranges are correct using the formula given, $n(n+1)/2$ but you have to subtract the result for the start and end for the subrange.

The formula would be:
```python
((end * (end+1)) / 2) - ((start * (start+1)) / 2)
```

Basically, a `final - initial` for the range.

Let's go ahead and do a calculation with that equation to make sure it's right:
``` {python}
start_1hunt = 100000
end_2hunt = 200000

subrange_sum_1hunt_to_2hunt = ((end_2hunt * (end_2hunt+1)) / 2) - ((start_1hunt * (start_1hunt+1))/2)
```

Which results in `{python} subrange_sum_1hunt_to_2hunt`.

Well, now that I do the calculation, there does seem to be a descrepancy between what we get with the calclulation vs. what we get from the program.

From the program, for the range we calculated here we get `gsum[1]=14999950000`, which is 100k less than what it actually should be.

Based on what is said in the comments in the program, I'm assuming it's because each "worker" sums up until `n-1` rather than up to their `n`.

Here is the code I'm referencing (I added the comment with the arrow):
```c
// we add int COUNT here in the final reduction since each worker
// summed up to n-1, so we have to add final value n

// this can probably be parallelized by openmp
#pragma omp parallel for num_threads(NUM_THREADS) reduction(+:gsumall)
for(i=0; i<NUM_THREADS; i++)
    gsumall+=gsum[i];

gsumall+=COUNT; // <--- this here is where we get that extra 100k in the end
```

## 2.

See `eratosthenes-parallel/eratosomp.c` for my implementation.

Here's an example output for the time:
```bash
$ ./eratosomp
Using DEFAULT thread_count of 20
max uint = 4294967295
max long long = 18446744073709551615
Sufficient memory for prime sieve up to 1000000000 using 119 Mbytes at address=0x7dded2474010
Initial set_isprime(1000000000) in 0.271367 secs
chk_isprime(1000000000) in 3.788116 secs
Entering cross out loop
Done with cross out loop with set_isprime(1000000000) time=1.919702 secs
Total number of primes chk_isprime(1000000000) in 0.160676 secs
Number of primes [0..1000000000]=50847534
```

Here's an output for working with semiprimes:
```bash
Example large SP is 20127306543107, factored into p1=1299721, p2=15485867
The largest prime in 1000000000 is 999999937
The next largest prime in 1000000000 is 999999929
Looking for the semiprimes using our prime list:
The primes were: 5, and 7 for 35
The primes were: 439, and 857 for 376223
The primes were: 46411, and 86323 for 4006336753
The primes were: 470303, and 864583 for 406615978649
The primes were: 47868193, and 86781887 for 4154092115820191
The primes were: 868719143, and 481346903 for 418155269059864129
```

As you can see, largest primes were 999999937 and then 999999929.

The random, example semiprime that I found using my primes was 20127306543107 with its primes I grabbed from the graph being 1299721 and 15485867.

As you can see I did go ahead and test the semiprimes that we were given in the documentation and my program successful reverse searched for their primes.

You'll have to check my logic in the program to verify that I didn't just hard code the values.

{::: {.callout-warning}
I guess I need to use a second source to verify my calculations.

I think I'll just ask an LLM how I'm supposed to do this.
:::}

``` {python}
eratos_openmp_time = 8.243
eratos_n_openmp_time = 14.786

eratos_openmp_speedup = 14.786 / 8.243
```

The speed up with parallelization was `{python} f"{eratos_openmp_speedup:.2f}"`x faster.

## 3.

See `matrotate.c` for my OpenMP rotation program.

In testing without OpenMP for rotating all the images, the program completed the to-be-parallelized portion in:
```bash
$ ./matrotate 
Starting the rotations...
Finished rotating.
The pgms were rotated in 0.110802 sec
```

While adding an OpenMP to that loop this is what I got:
```bash
$ ./matrotate 
Starting the rotations...
Finished rotating.
The pgms were rotated in 0.035044 secs
```

``` {python}
n_parallel_rotation_t = 0.110802
parallel_rotation_t = 0.035044

rotation_speedup = n_parallel_rotation_t / parallel_rotation_t
```

Which is a `{python} f"{rotation_speedup:.2f}"`x speedup!!!
